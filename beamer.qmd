---
title: Integrating Poisson regression into the undergraduate curriculum
subtitle: USCOTS25 Breakout Session B3H
author: Laura Boehm Vock and Paul Roback
format: 
  beamer:
    navigation: horizontal
    urlcolor: blue
    header-includes: |
      \titlegraphic{\includegraphics[width=0.4\paperwidth]{StOlaf.jpg}}
---

```{r}
#| message: FALSE
#| warning: FALSE

# Packages required for Chapter 4
library(multcomp)
library(kableExtra)
library(MASS)
library(pscl)
library(tidyverse)
```

## A quick initial survey!

Please [click here](https://forms.gle/AWDgRuwTHW7YfLDZ9) or use the following QR code:

```{r out.width="50%", out.height="50%"}
#| echo: FALSE

knitr::include_graphics("qr-code.png")
```


## Poisson regression at St. Olaf

-   2002-04: Not taught.  Statistics concentration required Prob Theory and Math Stat plus 2 electives.
-   2004-18: Taught as part of Advanced Statistical Modeling (Stat 316).  Concentration required Statistical Modeling (Stat 272) and 316 plus 2 electives.
-   2018-24: Still taught in Stat 316.  Concentration renamed "Statistics and Data Science" and required 272 and Intro to Data Science plus 2 electives.  Stat 316 now counts as an upper level elective.
-   2024-now: Still taught in Stat 316.  Concentration became a major.  Stat 316 counts as a "Level 3 Stats Depth" elective course.


## Advanced Statistical Modeling at St. Olaf

-   Covers generalized linear models (Poisson regr, binomial regr, negative binomial regr, zero-inflated models, hurdle models, etc.) and multilevel modeling
-   Prerequsites: Intro Stats and Stat Modeling (nothing else -- calculus, linear algebra, computing, ...)
-   Applied focus using R
-   Uses [Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R](https://bookdown.org/roback/bookdown-BeyondMLR/) by Roback and Legler.  Second edition by Roback, Boehm Vock, and Legler expected by Fall 2026.


## Initial survey results


## First case study: Philippine households

```{r}
#| include: FALSE

fHH1 <- read_csv("data/philippines.csv") |>
  mutate(location = as_factor(location),
         roof = as_factor(roof),
         age2 = age*age,
         visayas = ifelse(location == "Visayas", 1, 0))
```

-   International agencies often use household size to determine the magnitude of the household needs
-   Want to discern factors associated with larger households
-   We will model both the total household size and number of children under 5
-   Data is subset from 2015 Philippine Statistics Authority's Family Income and Expenditure Survey (FIES)
-   Primary response is a count, which can make linear regression problematic


## Philippine household data

:::: {.columns}

::: {.column width="50%"}
Key variables:

-   `location` = region (Central Luzon, Davao, Ilocos, Metro Manila, or Visayas)
-   `age` = the age of the head of household
-   `total` = the number of people in the household other than the head
-   `numLT5` = the number in the household under 5 years of age 
-   `roof` = the type of roof (stronger material can be used as a proxy for greater wealth)
:::

::: {.column width="50%"}
```{r}
#| echo: FALSE

knitr::include_graphics("data/map_of_philippines.jpg")
```
:::

::::


## What is the Poisson *distribution*?

$$
P(Y_i=y_i) = \frac{e^{-\lambda}\lambda^{y_i}}{{y_i}!} \quad \textrm{for} \quad y_i = 0, 1, \ldots, \infty,
$$

Note that both $E(Y_i) = \lambda_i$ and $Var(Y_i) = \lambda_i$.

```{r}
#| label: fig-multPois
#| fig-cap: |
#|   Poisson distributions with $\lambda = 0.5,\ 1$, and $5$. 
#| fig-subcap: 
#|   - $\lambda = 0.5$
#|   - $\lambda = 1$
#|   - $\lambda = 5$
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| layout-ncol: 3
#| layout-nrow: 1
#| out-width: 70%

#poissonPlots
plotPois <- function(lam){
  yp = 0:10000 # possible values
  pd <- data.frame(x = rpois(yp, lam))  # generate random deviates
  breaks <- pretty(range(pd$x), n = nclass.FD(pd$x), min.n = 1)  
  # pretty binning
  #bwidth <- breaks[2] - breaks[1]
  ggplot(pd, aes(x = x)) + 
    geom_histogram(aes(y = ..count.. / sum(..count..)), 
                   binwidth = .25) +
    xlab("number of events") + 
    ylab("probability") + 
    xlim(-1,13)
}

Pois1 <- plotPois(0.5)
Pois2 <- plotPois(1)
Pois3 <- plotPois(5) + scale_y_continuous(breaks = c(0, 0.1))
Pois1
Pois2
Pois3
```


## What is a Poisson *regression model*?

\begin{equation*}
log(\lambda_i)=\beta_0+\beta_1 x_i
\end{equation*}
where the observed values $Y_i \sim$ Poisson with $\lambda=\lambda_i$ for a given $x_i$. 


#### Poisson model conditions:

1. __Poisson Response__ The response variable is a count per unit of time or space, described by a Poisson distribution.
2. __Independence__ The observations must be independent of one another.
3. __Mean=Variance__ By definition, the mean of a Poisson random variable must be equal to its variance.
4. __Linearity__ The log of the mean rate, log($\lambda$), must be a linear function of x.


## Poisson regression conditions: A graphical look

```{r}
#| label: fig-OLSpois
#| fig-cap: |
#|   Comparison of regression models. 
#| fig-subcap: 
#|   - Linear Regression
#|   - Poisson Regression
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| layout-ncol: 2
#| layout-nrow: 1
#| out-width: 90%


## Sample data for graph of OLS normality assumption
## Code from https://stackoverflow.com/questions/31794876/ggplot2-how-to-curve-small-gaussian-densities-on-a-regression-line?rq=1

set.seed(0)
dat <- data.frame(x = (x = runif(10000, 0, 50)),
                  y = rnorm(10000, 10 * x, 100))

## breaks: where you want to compute densities
breaks <- seq(0, max(dat$x), len=5)
dat$section <- cut(dat$x, breaks)

## Get the residuals
dat$res <- residuals(lm(y ~ x, data=dat))

## Compute densities for each section, flip the axes, add means 
## of sections.  Note: densities need to be scaled in relation 
## to section size (2000 here)
dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(x$res, n = 5000)
  res <- data.frame(x = max(x$x)- d$y * 1000, y = d$x + mean(x$y))
  res <- res[order(res$y), ]
  ## Get some data for normal lines as well
  xs <- seq(min(x$res), max(x$res), len = 5000)
  res <- rbind(res, 
               data.frame(y = xs + mean(x$y),
                          x = max(x$x) - 1000*dnorm(xs, 0, sd(x$res))))
  res$type <- rep(c("empirical", "normal"), each = 5000)
  res
}))
dens$section <- rep(levels(dat$section), each = 10000)

ols_assume <- ggplot(dat, aes(x, y)) +
  geom_point(size = 0.1, alpha = .25) +
  geom_smooth(method = "lm", fill = NA, lwd = 2) +
  geom_path(data = dens[dens$type == "normal",], 
            aes(x, y, group = section), 
            color = "salmon", 
            lwd = 1.1) +
  theme_bw() +
  geom_vline(xintercept = breaks, lty = 2)

# Now make Poisson regression picture
set.seed(0)
dat <- data.frame(x = (x = runif(1000, 0, 20)),
                  y = rpois(1000, exp(.1 * x)))

## breaks: where you want to compute densities
breaks <- seq(2, max(dat$x), len = 5)
dat$section <- cut(dat$x, breaks)

## Get the residuals
dat$res <- dat$y - .1 * dat$x

## Compute densities for each section, flip the axes, add means
## of sections.  Note: densities need to be scaled in relation 
## to section size
dens <- do.call(rbind, lapply(split(dat, dat$section), function(x) {
  d <- density(x$res, n = 500)
  res <- data.frame(x = max(x$x)- d$y * 10, y = d$x + mean(x$y))
  res <- res[order(res$y), ]
  ## Get some data for poisson lines as well
  xs <- seq(min(x$y), max(x$y), len = 500)
  res <- rbind(res, 
               data.frame(y = xs,
                          x = max(x$x) - 
                            10*dpois(round(xs), exp(.1*max(x$x)))))
  res$type <- rep(c("empirical", "poisson"), each = 500)
  res
}))
dens$section <- rep(levels(dat$section), each = 1000)

pois_assume <- ggplot(dat, aes(x, jitter(y, .25))) +
  geom_point(size = 0.1) +
  geom_smooth(method = "loess", fill = NA, lwd = 2) +
  geom_path(data=dens[dens$type=="poisson",], 
            aes(x, y, group = section), 
            color = "salmon", 
            lwd = 1.1) +
  theme_bw() + 
  ylab("y") + 
  xlab("x") +
  geom_vline(xintercept = breaks, lty = 2)

ols_assume
pois_assume
```


## Pause to Ponder

With your neighbor(s), compare the Poisson regression conditions to the usual LINE conditions in linear regression.  List similarities and differences.  What implications might the differences have for modeling and checking conditions?


## Differences with linear regression (LLSR)

1. For each level of X, the responses follow a Poisson distribution (Condition 1). For Poisson regression, small values of $\lambda$ are associated with a distribution that is noticeably skewed with lots of small values and only a few larger ones. As $\lambda$ increases the distribution of the responses begins to look more and more like a normal distribution.
2. In the LLSR model, the variation in $Y$ at each level of X, $\sigma^2$, is the same. For Poisson regression the responses at each level of X become more variable with increasing means, where variance=mean (Condition 3). 
3. In the case of LLSR, the mean responses for each level of X, $\mu_{Y|X}$, fall on a line. In the case of the Poisson model, the mean values of $Y$ at each level of $X$, $\lambda_{Y|X}$, fall on a curve, not a line, although the logs of the means should follow a line (Condition 4).


## Side by side comparison

Linear Regression | Poisson Regression
------------------+--------------------
$Y \sim N(\mu, \sigma)$       | $Y \sim Pois(\lambda)$
$\mu = \beta_0 + X\beta_1$ | $log(\lambda) = \beta_0 + X\beta_1$
**L**inear relationship of X and Y | Log linear relationship of X and E(Y)
**I**ndependent observations | Independent observations
**N**ormally distributed residuals  | Poisson distributed Y
**E**qual variance of all residuals | Variance increases with E(Y) (Var(Y) = E(Y))


## Exploratory data analysis 1: Number under 5 in household

```{r}
#| label: fig-eda-lt5
#| fig-cap: |
#|   EDA: selected plots 
#| fig-subcap: 
#|   - Distribution of number of children under 5 in households across all 5 Philippine regions.
#|   - The log of the mean number of children under 5 by age of the head of household, with loess smoother.
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| layout-ncol: 2
#| layout-nrow: 1
#| out-width: 70%

LT5_dist <- ggplot(fHH1, aes(numLT5)) + 
  geom_histogram(binwidth = .25, 
                 color = "black", 
                 fill = "white") + 
  xlab("Number in the household excluding the head") +
  ylab("Count of households")

## Checking linearity assumption: Empirical log of the means plot
sumStats <- fHH1 |> 
  group_by(age) |> 
  summarise(mnLT5 = mean(numLT5),
            logmnLT5 = log(mnLT5), 
            n=n())

age_vs_LT5 <- ggplot(sumStats, aes(x=age, y=logmnLT5)) +
  geom_point() +
  geom_smooth(method = "lm")+
  xlab("Age of head of the household") +
  ylab("Log mean # of children < 5") 

LT5_dist + theme(axis.title.x = element_text(size = 30, face = "bold"),
                 axis.title.y = element_text(size = 30, face = "bold"))
age_vs_LT5 + theme(axis.title.x = element_text(size = 30, face = "bold"),
                 axis.title.y = element_text(size = 30, face = "bold"))
```


## Initial model

\begin{equation*}
log(\hat{\lambda}) = 0.525 - 0.029 \textrm{age}
\end{equation*}

```{r}
modelaLT5 <- glm(numLT5 ~ age, family = poisson, data = fHH1)
```

\scriptsize
```{r}
#| echo: FALSE
#| message: FALSE
#| comment: '##'

coef(summary(modelaLT5))
```

\normalsize
**Pause to Ponder:** How would you want your students to interpret coefficient estimates above? 


## Interpreting model coefficients

If your students have interpreted coefficients with a log-transformed response in linear regression, or log odds in logistic regression, this is similar.

Consider how the estimated mean number in the house, $\lambda$, changes as the age of the household head increases by an additional year. 

\small
$$
\begin{aligned}
log(\lambda_X) &= \beta_0 + \beta_1X \\
log(\lambda_{X+1}) &= \beta_0 + \beta_1(X+1) \\
log(\lambda_{X+1})-log(\lambda_X) &=  \beta_1 \\
log \left(\frac{\lambda_{X+1}}{\lambda_X}\right)   &= \beta_1\\
\frac{\lambda_{X+1}}{\lambda_X} &= e^{\beta_1}
\end{aligned}
$$ {#eq-rateRatio}

\normalsize
These results suggest that by exponentiating the coefficient on age we obtain the *multiplicative* factor by which the mean count changes. 


## Interpreting model coefficients (continued)

In this case, the mean number of children under 5 changes by a factor of $e^{-0.029}=0.971$ or decreases by 2.9\% (since $1-.971 = .029$) with each additional year older the household head is; 

Or, we predict a 3.0\% *increase* in mean number of children less than 5 for a 1-year *decrease* in age of the household head (since $1/.971=1.0298$). 

The quantity on the left-hand side of @eq-rateRatio is referred to as a __rate ratio__  or __relative risk__, \index{relative risk (rate ratio)} and it represents a percent change in the response for a unit change in X.  


## Multiple Poisson regression

Most significant ideas from multiple linear regression are reinforced by Poisson regression:

-   hypothesis testing
-   confidence intervals
-   indicator variables
-   categorical variables (reference level / Tukey HSD)
-   squared terms
-   interaction terms
-   control for / adjusting for / holding constant
-   checking violations of model conditions
-   maximum likelihood estimates (equals least squares in linear regr)


## Exploratory data analysis 2: Total in household

```{r}
#| label: fig-eda-total
#| fig-cap: |
#|   EDA: selected plots 
#| fig-subcap: 
#|   - Distribution of household size by Philippine regions.
#|   - The log of the mean household sizes by age of the head of household, with loess smoother.
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| layout-ncol: 2
#| layout-nrow: 1
#| out-width: 70%

textdat <- fHH1 |> 
  group_by(location) |>
  summarize(mean = round(mean(total), 2), 
            var = round(var(total), 2) ) |>
  mutate(Mean = str_c("Mean: ", mean, "\nVar: ", var))


total_dist <- ggplot(fHH1, aes(total)) + 
  geom_histogram(binwidth = .25, 
                 color = "black", 
                 fill = "white") + 
  xlab("Number in the house excluding head of household") +
  ylab("Count of households") +
  facet_wrap(~location) +
  geom_text(data = textdat, aes(x = 10, y = 90, label = Mean))
 

## Checking linearity assumption: Empirical log of the means plot
sumStats <- fHH1 |> 
  group_by(age) |> 
  summarise(mntotal = mean(total),
            logmntotal = log(mntotal), 
            n=n())

age_vs_total <- ggplot(sumStats, aes(x=age, y=logmntotal)) +
  geom_point()+
  geom_smooth(method = "loess", linewidth = 1.5)+
  xlab("Age of head of the household") +
  ylab("Log of the empirical mean number in the house") 

total_dist
age_vs_total
```

What does the graph on the right tell us about the relationship of $\lambda$ and age?


## Potential final model

```{r}
modela2 <- glm(total ~ age + age2, family = poisson, data = fHH1)
```

\scriptsize
```{r}
#| echo: TRUE

modela2L <- glm(total ~ age + age2 + location, family = poisson, data = fHH1)
```

```{r}
#| echo: FALSE
#| message: FALSE
#| comment: '##'

coef(summary(modela2L))
```

\small
For example, $\hat{\beta}_6=-0.0194$ indicates that, after controlling for the age of the head of household, the log mean household size is 0.0194 lower for households in the Davao Region than for households in the reference location of Central Luzon. 

In more interpretable terms, mean household size is $e^{-0.0194}=0.98$ times "higher" (i.e., 2\% lower) in the Davao Region than in Central Luzon, when holding age constant.  

Maximum estimated additional number in the house occurs when the head of the household is around 50 years old, after adjusting for location.


## Potential final model (continued)

\normalsize
To test for the effect of location, use a drop-in-deviance test (analogous to an extra-sum-of-squares F test in linear regression):

```{r}
#| message: FALSE

drop_in_dev <- anova(modela2, modela2L, test = "Chisq")
```

```{r}
#| echo: FALSE
#| message: FALSE
#| comment: '##'

did_print <- data.frame(
  ResidDF = drop_in_dev$`Resid. Df`,
  ResidDev = drop_in_dev$`Resid. Dev`,
  Deviance = drop_in_dev$Deviance, 
  Df = drop_in_dev$Df,
  pval = drop_in_dev$`Pr(>Chi)`
)
row.names(did_print) <- row.names(drop_in_dev)
did_print
```

Adding the four terms corresponding to location to the quadratic model with age produces a statistically significant improvement $(\chi^2=13.144, df = 4, p=0.0106)$, so there is significant evidence that mean household size differs by location, after controlling for age of the head of household. 


## Comparing non-nested models

```{r}
model_roof <- glm(total ~ roof, family = poisson, data = fHH1)
model_location <- glm(total ~location, family = poisson, data = fHH1)
```

Which is the single best predictor? quadratic age, roof type, or location/region? 

Use Akaike Information Criterion (AIC) to compare

-   lower values are better
-   differences of 10 are "big"
-   measures model fit while accounting for complexity, analog to adjusted $R^2$

```{r}
#| message: FALSE

AIC(modela2, model_roof, model_location)
```


## Lack of fit!

When a model is true, we can expect the residual deviance to be distributed as a $\chi^2$ random variable with degrees of freedom equal to the model's residual degrees of freedom.  

Our final model has a residual deviance of 2187.8 with 1493 df. The probability of observing a deviance this large if the model fits is essentially 0, saying that there is significant evidence of lack-of-fit. 

There are several reasons why **lack-of-fit** \index{lack-of-fit} may be observed:

-   We may be missing important covariates or interactions.  
-   There may be extreme observations that cause the deviance to be larger than expected. 
-   There may be a problem with the Poisson model.  In particular, the Poisson model has only a single parameter, $\lambda$, for each combination of the levels of the predictors which must describe both the mean and the variance.


## Overdispersion

Often in Poisson models the variances in the response are larger than the corresponding means at different levels of the predictors.  The response is then considered to be **overdispersed**.

Recall that we observed this in our EDA plot by region:

```{r}
#| label: fig-lt5-by-region
#| fig-cap: |
#|   Distribution of household size by Philippine regions.
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| out-width: 70%

textdat <- fHH1 |> 
  group_by(location) |>
  summarize(mean = round(mean(total), 2), 
            var = round(var(total), 2) ) |>
  mutate(Mean = str_c("Mean: ", mean, "\nVar: ", var))


total_dist <- ggplot(fHH1, aes(total)) + 
  geom_histogram(binwidth = .25, 
                 color = "black", 
                 fill = "white") + 
  xlab("Number in the house excluding head of household") +
  ylab("Count of households") +
  facet_wrap(~location) +
  geom_text(data = textdat, aes(x = 10, y = 90, label = Mean, size = 30))

total_dist + theme(axis.title.x = element_text(size = 18, face = "bold"),
                   axis.title.y = element_text(size = 18, face = "bold"))
```


## Quasi-Poisson models

Without adjusting for overdispersion, we use incorrect, artificially small standard errors leading to artificially small p-values for model coefficients. 

The simplest way to take overdispersion into account is to use an estimated dispersion factor to inflate standard errors.
 
$\hat\phi=\frac{\sum(\textrm{Pearson residuals})^2}{n-p}$ where $p$ is the number of model parameters. 

$SE_Q(\hat\beta)=\sqrt{\hat\phi}*SE(\hat\beta)$


## Quasi-Poisson models (continued)

\scriptsize
```{r}
modela2L_quasi <- glm(total ~ age + age2 + location,
                      family = quasipoisson, 
                      data = fHH1)
```

```{r}
#| echo: FALSE
#| message: FALSE
#| comment: '##'

coef(summary(modela2L_quasi))
cat(" Residual deviance = ", summary(modela2L_quasi)$deviance, " on ",
    summary(modela2L_quasi)$df.residual, "df", "\n",
    "Dispersion parameter = ", summary(modela2L_quasi)$dispersion)
```

\normalsize
In the absence of overdispersion, we expect the dispersion parameter estimate to be 1.0. The estimated dispersion parameter here is larger than 1.0 (1.415).

For example, the standard error for the Visayas region term from a likelihood based approach is 0.0417, whereas the quasi-likelihood standard error is $\sqrt{1.415}*0.0417$ or 0.0497.  This term is still statistically significant at the 0.05 level under the quasi-Poisson model, but the evidence is not as strong (quasi-Poisson p-value of .024 vs. Poisson p-value of .007). 

Furthermore, tests for individual parameters are now based on the t-distribution rather than a standard normal distribution, with test statistic $t=\frac{\hat\beta}{SE_Q(\hat\beta)}$ following an (approximate) t-distribution with $n-p$ degrees of freedom if the null hypothesis is true ($H_O:\beta=0$). 

Drop-in-deviance tests can be similarly adjusted for overdispersion in the quasi-Poisson model.  $F=\frac{\textrm{drop in deviance}}{\textrm{difference in df}} / {\hat\phi}$ follows an (approximate) F-distribution when the null hypothesis is true ($H_0$: reduced model sufficient).  


## Quasi-Poisson models (continued)

We can take another look at our final model:

```{r}
#| message: FALSE

modela2_quasi <- glm(total ~ age + age2,
                     family = quasipoisson, 
                     data = fHH1)
drop_in_dev <- anova(modela2_quasi, modela2L_quasi, test = "F")
```

\scriptsize
```{r}
#| echo: FALSE
#| message: FALSE
#| comment: '##'

did_print <- data.frame(
  ResidDF = drop_in_dev$`Resid. Df`,
  ResidDev = drop_in_dev$`Resid. Dev`,
  F = drop_in_dev$F, 
  Df = drop_in_dev$Df,
  pval = drop_in_dev$`Pr(>F)`
)
row.names(did_print) <- row.names(drop_in_dev)
did_print
```

\normalsize
Here, after adjusting for overdispersion, we find that there is *not* statistically significant evidence at the 0.05 level ($F=2.32, p=.055$) that mean household size differs among regions after adjusting for age.  


## Alternative methods for modeling overdispersion

Diagnostic plot for overdisperion: plot mean vs. variance for groups of households based on predicted size

-   linear with slope > 1 => quasi-Poisson
-   quadratic with incr. slope => negative binomial

```{r}
#| label: fig-mean-var2
#| fig-cap: |
#|   Mean and variance of predicted household sizes.  
#| fig-subcap: 
#|   - Linear fit
#|   - Loess smoother
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| layout-ncol: 2
#| layout-nrow: 1
#| out-width: 70%

final_qp <- fHH1 |> 
  mutate(pred = predict(modela2L), 
         grouping = cut_number(pred, 30)) |>
  group_by(grouping) |>
  summarize(meantotal = mean(total), 
            vartotal = var(total)) |>
  ggplot(aes(x = meantotal, y = vartotal)) +
    geom_point() + 
    geom_smooth(method = "lm") + 
    geom_abline(slope = 1, intercept = 0) +
    labs(x = "Mean household size",
         y = "Variance in household size")

final_nb <- fHH1 |> 
  mutate(pred = predict(modela2L), 
         grouping = cut_number(pred, 30)) |>
  group_by(grouping) |>
  summarize(meantotal = mean(total), 
            vartotal = var(total)) |>
  ggplot(aes(x = meantotal, y = vartotal)) +
    geom_point() + 
    geom_smooth(span = 2) + 
    geom_abline(slope = 1, intercept = 0) +
    labs(x = "Mean household size",
         y = "Variance in household size")

final_qp
final_nb
```


## Negative Binomial models

A negative binomial model introduces another parameter in addition to $\lambda$, which gives the model more flexibility and, as opposed to the quasi-Poisson model, the negative binomial model assumes an explicit likelihood model.  

Mathematically, you can think of the negative binomial model as a Poisson model where $\lambda$ is also random, following a gamma distribution. 

These results are very similar to the quasi-Poisson model in terms of estimated coefficients (which can change), standard errors, test statistics, and p-values.


## Pause to Ponder

Check in with your neighbor(s).  What questions do you have at this point?  What can we clarify or discuss more fully?


## Second case study: Bald eagles

```{r}
#| include: FALSE

# Load eagles data
eagles <- read_csv("data/bald_eagles.csv") |>
  mutate(weeks = hours / 168,
         log_count_per_week = log(count_per_week + 0.5),
         year_1981 = year - 1981)
```

Every year in late December, since 1921, birdwatchers in the Hamilton area of Ontario, Canada, have counted and recorded all the birds they see or hear in a day.

The data was made available by the [Bird Studies Canada website](https://www.birdscanada.org/) and distributed through the R for Data Science TidyTuesday project.  

We are particularly interested in how the Bald Eagle population has changed over time. 


## Bald eagle data

:::: {.columns}

::: {.column width="60%"}
Each row of `bald_eagles.csv` contains information about bald eagles counts in Hamilton, Ontario, for one year.  There are 37 rows covering 1981 through 2017.  The variables include:

- `year` = year of data collection
- `count` = number of birds observed
- `hours` = total person-hours of observation period
- `count_per_hour` = count divided by hours
- `count_per_week` = count_per_hour multiplied by 168 hours per week
:::

::: {.column width="40%"}
```{r}
#| echo: FALSE

knitr::include_graphics("data/Bald-Eagle.jpg")
```

\tiny
Credit: © Ron Niebrugge/wildnatureimages
:::

::::


## Exploratory data analysis

\normalsize
```{r}
#| label: fig-eda-eagle
#| fig-cap: |
#|   EDA: selected plots 
#| fig-subcap: 
#|   - Histogram of number of bald eagle sightings by year.
#|   - Bald eagle counts vs. year
#|   - Hours of observation vs. year
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| layout-ncol: 3
#| layout-nrow: 1
#| out-width: 70%

count_dist <- ggplot(eagles, aes(x = count)) + 
  geom_histogram(bins = 15, color = "black", fill = "white") +
  xlab("Number of bald eagles")

count_by_year <- ggplot(eagles, aes(x = year, y = count)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  xlab("Year") +
  ylab("Bald eagle count")

obs_by_year <- ggplot(eagles, aes(x = year, y = weeks)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  xlab("Year") +
  ylab("Weeks of observation")

count_dist
count_by_year
obs_by_year
```


## Sampling effort

Poisson random variables are often used to represent counts (e.g., number of bald eagles) *per unit of time or space* (previously, number of people in one household).  

But what if observation (sampling) effort (as measured by the number of `weeks` people observed birds) is changing over time?

We cannot directly compare the 2 eagles observed in 1985 to the 7 eagles observed in 2015 when there were only 143 person-hours (0.85 person-weeks) of observation in 1985 compared with 221 person-hours (1.32 person-weeks) in 2015. 

We should examine time trends in the *rate* of bald eagles sightings; for example, we will calculate the bald eagle counts per week ($\frac{\textrm{number of bald eagles}}{\textrm{hours of observation}} \cdot (168 \textrm{ hours/week})$).  


## Offsets

We let $\lambda_i$ be the expected number of eagles in year $i$ per with $\textrm{weeks}_i$ weeks observed in year $i$. 

Then $\lambda_i/\textrm{weeks}_i$ is the number of eagles expected in a week!

Adjusting the yearly count by observation time is equivalent to adding $log(\textrm{weeks})$ to the right-hand side of the Poisson regression equation---essentially adding a predictor with a fixed coefficient of 1, called an __offset__: 

\begin{align*} 
log(\frac{\lambda_i}{\textrm{weeks}_i} )= \beta_0 + \beta_1x_i \nonumber \\
log(\lambda_i)-log(\textrm{weeks}_i) = \beta_0 + \beta_1x_i \nonumber \\
log(\lambda_i) = \beta_0 + \beta_1x_i + log(\textrm{weeks}_i)
\end{align*}

Thus, modeling $log(\lambda)$ and adding an offset is equivalent to modeling **rates**, and coefficients can be interpreted in terms of rates.


## Interpretation 

Ordinary Poisson: 

$$log(\mbox{COUNT}) = log(\lambda) =\beta_0 + \beta_1 X$$

The average *number of eagles* has increased by ...% per year. 

Poisson with Offset: 

$$log(\mbox{RATE}) = log(\lambda/t) = \beta_0 + \beta_1 X$$
The average *number of eagles per week* has increase by ...% per year. 


## Modeling results

We are interested primarily in trends over time in eagle sightings.  We have no control variables other than sampling effort, so we simply fit a model with `year` (centered at 1981) and our offset.

\scriptsize
```{r}
#| echo: TRUE

model_eagles <- glm(count ~ year_1981, family = poisson,
                    offset = log(weeks), data = eagles)
```

```{r}
#| echo: FALSE
#| message: FALSE
#| comment: '##'

coef(summary(model_eagles))
cat(" Residual deviance = ", summary(model_eagles)$deviance, " on ",
    summary(model_eagles)$df.residual, "df", "\n",
    "Dispersion parameter = ", summary(model_eagles)$dispersion)
```

\normalsize
Bald eagle counts are significantly increasing over time (Z = 6.55, p < .001), even after adjusting for observation time.  The average eagle sighting rate per week has grown about 7.9\% per year (since $e^{0.0757}=1.0786$) in Hamilton, Ontario.  

Adjustments for potential overdispersion using either quasi-Poisson or negative binomial regression provide minimal changes to model coefficients and tests.


## Other extensions of the Poisson regression model

-   Zero-inflated models
-   Hurdle models
-   Multilevel models


## Zero-inflated Poisson models: Weekend drinking

An informal survey of students in an intro stats course included the question, "How many alcoholic drinks did you consume last weekend?".  We wish to identify factors associated with increased drinking.

```{r}
#| label: fig-zip
#| fig-cap: |
#|   Count of drinks consumed last weekend
#| fig-subcap: 
#|   - Actual distribution
#|   - Poisson distribution with same mean
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| layout-ncol: 2
#| layout-nrow: 1
#| out-width: 70%

# Read in weekendDrinks data
zip_data <- read_csv("data/weekendDrinks.csv")  

## Observed data
obs_table <- tally(group_by(zip_data, drinks))  |>
  mutate(prop = round(n / sum(n), 3))
# 47% reported 0 drinks last weekend

g_obs <- obs_table |>
  ggplot(aes(y = prop, x = drinks)) + 
    geom_bar(stat = "identity") +
    labs(x = "Number of drinks", y = "Proportion") +
    coord_cartesian(ylim = c(0, .5))
g_obs

## Poisson model
### lambda = mean number of drinks
sum1 <- zip_data |> 
  summarise(lambda = mean(drinks),
            maxDrinks = max(drinks))
possible_values <- with(sum1, 0:maxDrinks)
model_prob <- with(sum1, dpois(possible_values, lambda))
pois_model <- data.frame(possible_values, model_prob)

g_model <- ggplot(pois_model, aes(y = model_prob,
                                  x = possible_values)) + 
  geom_bar(stat = "identity")+
  labs(x = "Number of drinks", y = "Probability") +
  coord_cartesian(ylim = c(0, .5))
g_model
```

There are more zeros than expected under a Poisson model.


## ZIP: Weekend drinking (continued)

\small
Our zeros are a **mixture** of responses from non-drinkers (who would always report 0) and drinkers who abstained during the past weekend.  Ideally, we'd like to sort out the non-drinkers and drinkers when performing our analysis.

Define $\lambda$ to be the mean number of drinks *among those who drink*, and $\alpha$ to be the proportion of *non-drinkers* ("true zeros").  

Model $\lambda$ and $\alpha$ (or functions of $\lambda$ and $\alpha$) simultaneously using covariates like sex, first-year status, and off-campus residence.  For example:

$log(\lambda)=\beta_0+\beta_1 offcampus + \beta_2 sex$

$log(\alpha / (1-\alpha))=\beta_0+\beta_1 firstyear$

The first part of a ZIP model is a regular Poisson regression model, and the second part is a logistic regression model.


## Hurdle models: Going vague

\normalsize
In a 2018 study, Chapp et al. scraped every issue statement from webpages of candidates for the U.S. House of Representatives, counting the number of issues candidates commented on and scoring the level of ambiguity of each statement.  

Research questions:

-   Which candidates for U.S. House are more likely to have at least one issue page and to offer statements on a greater number of issues?  
-   How are a candidate's political party, incumbency status, and political beliefs related to their willingness to post stands and ideas on issues?  
-   How do the demographics and political beliefs of voters in the candidate's district impact a candidate's willingness to engage?  
-   How does the interplay between candidate profile and voter profile affect a candidate's willingness to comment on issues?


## Hurdle: Going vague (continued)

```{r}
#| label: fig-hurdle
#| fig-cap: |
#|   Total issue pages for 2014 US House candidates
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
#| out-width: 50%
#| out-height: 30%

ambiguity <- read_csv("data/ambiguity.csv") |>
  mutate(democrat_fct = ifelse(democrat == 1, "Democrat", "Republican"),
         incumbent_fct = ifelse(incumbent == 1, "Incumbent", "Challenger"),
         issue_pages = ifelse(totalIssuePages == 0, 
                              "No issue pages",
                              "At least one issue page"))

p.table <- ambiguity |>
  group_by(totalIssuePages) |>
  summarise (n = n()) |>
  mutate(freq = n / sum(n))

ggplot(p.table, aes(x = totalIssuePages, 
                    xend = totalIssuePages, 
                    y = 0, 
                    yend = freq)) +
  geom_segment() + 
  labs(y = "Proportion", 
       x = "Observed total issue pages")
```

\small
Once again, there are more zeros than expected under a Poisson model.  But *unlike* ZIP models, it is not natural to consider these zeros to be a mixture.  Candidates decide either to post issue statements or not.  Those who decide to not post any issue statements comprise our zeros, and for those who decide to post issue statements, we can model the number they choose to post.  

Since those who decide to post issue statements "leap over" the zero category, these models are referred to as __hurdle models__.


## Hurdle: Going vague (continued)

\small
Similar to ZIP models, we will define $\lambda$ to be the mean number of drinks *among those who posted at least one issue page*, and $\alpha$ to be the proportion of *candidates who post at least one issue page* ("true non-zeros" or "true hurdlers").  

Then, model $\lambda$ and $\alpha$ (or functions of $\lambda$ and $\alpha$) simultaneously using characteristics of the candidates and their districts:

$log(\lambda)=\beta_0+\beta_1 X_1 + \beta_2 X_2$

$log(\alpha / (1-\alpha))=\beta_0+\beta_1 X_1 + \beta_2 X_3$

Hurdle models focus on modeling *non-zeros*, whereas ZIP models focus on modeling *true zeros*.  This really only affects the interpretation of logistic coefficients.

The count portion of the hurdle model is actually based on a **truncated Poisson** distribution (domain starting at 1) rather than a full Poisson distribution (domain starting at 0).  This does not affect the interpretation of Poisson coefficients.


## Multilevel modeling: Going vague

\normalsize
**Problem:** Even with a hurdle model for number of issue pages, a condition is still violated in the Going Vague example.  Observations are *not independent*!  Some covariates are measured at the candidate level (incumbent, party, ideology), while others are measured at the district level (demographics, ideology of voters).  Candidates from the same district will have the same values for any covariate at the district level.

**Implications:** overstate effective sample size, underestimate standard errors, and overstate significance of covariates

**Solution:** multilevel (hierarchical / mixed effects) modeling!

**Idea:** build a regression model for issue pages at the candidate level, and then build another regression model for coefficients from the first model using covariates at the district level.  Combine into a single composite model.


## Pause to Ponder

With your neighbor(s) discuss plans, ideas, questions, and concerns you have about teaching Poisson regression somewhere in your own world.


## Preview of materials

[GitHub repo](https://github.com/proback/USCOTS25_Poisson_breakout) for this session, containing:

-   slides for this presentation (including R source code and data)
-   St. Olaf SDS 316 class folder with class activities for the Poisson regression unit (keys available upon request).
-   Draft of brand new Chapter 4 for BMLR2e (Roback, Boehm Vock, and Legler; expected Fall 2026)

[Beyond Multiple Linear Regression: Applied Generlized Linear Models and Multilevel Models in R](https://bookdown.org/roback/bookdown-BeyondMLR/) by Roback and Legler (2021).


## Thanks!

Please be in touch with any questions, thoughts, feedback, etc.!

Laura Boehm Vock: (boehm@stolaf.edu)

Paul Roback: (roback@stolaf.edu)


## Bonus material: Interpreting ZIP

```{r}
zip_data <- zip_data|>
  mutate(offcampus = ifelse(dorm == "off campus", 1, 0),
         firstyear = ifelse(dorm %in% c("kildahl", "kittlesby", "mohn"), 1, 0))

drinks.zip <- zeroinfl(drinks ~  offcampus + sex | 
                     firstyear,
                   data=zip_data)
```

```{r}

summary(drinks.zip)$coef
```

$e^{0.415} = 1.51$: Students who drink consume 51% more drinks if living off campus, after accounting for sex.

$e^{1.136} = 3.11$: First year students have 3.11 times greater odds of *not drinking* compared to older students.


## Bonus material: Interpreting hurdle models

```{r}
#| include: FALSE

hurdle_model <- hurdle(totalIssuePages ~ democrat + incumbent + 
    demHeterogeneity + mismatch + attHeterogeneity + distLean + 
    ideology + ideology:democrat | democrat + incumbent +
    demHeterogeneity + mismatch + attHeterogeneity + distLean + 
    ideology + ideology:democrat, 
  data = ambiguity, 
  dist="poisson", 
  zero="binomial")

summary(hurdle_model)$coef
```

\small
**incumbent:** $e^{-1.2085}=0.299$ and $1/e^{-1.2085}=3.35$.  Thus, the odds a challenger posts at least one issue page are 3.35 times greater than the odds an incumbent posts at least one issue page, holding all else constant.

**demHeterogeneity:** $e^{-1.173}=0.309$ and $1/e^{-1.173}=3.23$.  Thus, among candidates who choose to post at least one issue page, the mean number of issue pages posted are 3.23 times greater with each one unit decrease in demographic heterogeneity score, holding all else constant.

**ideology:democrat:** $1/e^{-0.490}=1.632$ and $1/e^{-0.490+0.222}=1.307$.  Thus, for each 1 unit decrease in ideology (more liberal), the mean number of issue pages (for candidates with at least one issue page) increases by 63.2\% for democrats but only 30.7\% for republicans.  

Overall candidates are disincentivized to take public stances on issues if they are an incumbent, if their constituents have a wide ranges of backgrounds, and if their beliefs are less aligned with their constituents.
